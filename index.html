<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Enhanced OoD Detection through Cross-Modal Alignment of Multi-modal Representations</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script
    async
    id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"
    type="text/javascript"
  ></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://ma-kjh.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://ieeexplore.ieee.org/abstract/document/10457104">
            Comparison of OoDD of CLIP based FT
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Enhanced OoD Detection through Cross-Modal Alignment of Multi-modal Representations</h1>
          <div class="is-size-4 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.co.kr/citations?user=u6DjYLsAAAAJ&hl=ko">Jeonghyeon Kim</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.kr/citations?user=QtI8XmgAAAAJ&hl=ko">Sangheum Hwang</a>,</span>
          </div>

          

          <div class="is-size-4 publication-authors">
            <span class="author-block">Seoul National University of Science and Technology</span>
          </div>
          <div class="is-size-4 publication-authors">
           <span class="author-block">🎉 CVPR 2025 🎉</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <!--- 채우기 --->
                <a href="" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <!--- 채우기 --->
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ma-kjh/CMA-OoDD"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="./static/images/teaser.jpg" alt="테저 이미지" style="max-width: 100%;">
      <h2 class="subtitle has-text-centered">
        By addressing the modality gap between image and text embeddings through cross-modal alignment (CMA)
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Prior research on out-of-distribution detection (OoDD) has primarily focused on single-modality models. 
            Recently, with the advent of large-scale pretrained vision-language models such as CLIP, OoDD methods utilizing such multi-modal representations through zero-shot and prompt learning strategies have emerged. 
            However, these methods typically involve either freezing the pretrained weights or only partially tuning them, which can be suboptimal for downstream datasets. 
            In this paper, we highlight that multi-modal fine-tuning (MMFT) can achieve notable OoDD performance. 
          </p>
          <p>
            Despite some recent works demonstrating the impact of fine-tuning methods for OoDD, there remains significant potential for performance improvement. 
            We investigate the limitation of na"ive fine-tuning methods, examining why they fail to fully leverage the pretrained knowledge. 
            Our empirical analysis suggests that this issue could stem from the modality gap within in-distribution (ID) embeddings.
          </p>
          <p>
            To address this, we propose a training objective that enhances cross-modal alignment by regularizing the distances between image and text embeddings of ID data.
            This adjustment helps in better utilizing pretrained textual information by aligning similar semantics from different modalities (i.e., text and image) more closely in the hyperspherical representation space. 
            We theoretically demonstrate that the proposed regularization corresponds to the maximum likelihood estimation of an energy-based model on a hypersphere. 
            Utilizing ImageNet-1k OoD benchmark datasets, we show that our method, combined with post-hoc OoDD approaches leveraging pretrained knowledge (e.g., NegLabel), significantly outperforms existing methods, achieving state-of-the-art OoDD performance and leading ID accuracy.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- 제목 -->
        <h2 class="title is-3">The Key Idea: Cross-Model Alignment</h2>

        <!-- figure: 세 개 이미지를 하나의 그룹으로 묶고, 공통 캡션 달기 -->
        <figure class="box" style="padding: 1rem;">
          <!-- 세 개의 이미지(좌우로 3열) -->
          <div class="columns">
            
            <!-- 첫 번째 이미지 컬럼 -->
            <div class="column">
              <img 
                src="./static/images/ZS_fig.jpg" 
                alt="Figure 1" 
                style="max-width: 100%; height: auto;"
              />
              <!-- 필요하다면, 각 이미지 아래에 짧은 부제목(소캡션)도 가능 -->
              <p style="margin-top: 0.5rem;">
                <small>Figure 1. ZS</small>
              </p>
            </div>
            
            <!-- 두 번째 이미지 컬럼 -->
            <div class="column">
              <img 
                src="./static/images/FLYP_fig.jpg" 
                alt="Figure 2" 
                style="max-width: 100%; height: auto;"
              />
              <p style="margin-top: 0.5rem;">
                <small>Figure 2. FLYP</small>
              </p>
            </div>
            
            <!-- 세 번째 이미지 컬럼 -->
            <div class="column">
              <img 
                src="./static/images/CMA_fig.jpg" 
                alt="Figure 3" 
                style="max-width: 100%; height: auto;"
              />
              <p style="margin-top: 0.5rem;">
                <small>Figure 3. CMA</small>
              </p>
            </div>
          </div>

          <!-- 전체 그림 그룹에 대한 공통 캡션 -->
          <figcaption class="has-text-centered" style="margin-top: 1rem;">
            <small>
              <strong>Above figures</strong> 세 개의 이미지를 묶은 공통 설명을 이곳에 작성할 수 있습니다.
            </small>
          </figcaption>
        </figure>
        
        <!-- 일반 본문 -->
        <div class="content has-text-justified">
          <p>
            The key idea behind CMA involves two main components: 
            1) alignment between the image and text modalities of the ID data to effectively
            separate negative text embeddings, and 
            2) correspondence between matching ID image-text pairs to maintain ID accuracy. To implement these ideas, we employ a contrastive loss for each modality with an additional CMA regularization loss. 
          </p>
        </div>

        <!-- 수식을 중앙에 배치할 DIV -->
        <div class="content has-text-centered" style="margin: 2rem 0;">
          <!-- (1) CLIP 수식 -->
          \[
            \mathcal{L}_{CLIP}
              = \frac{1}{2B}\sum_{k=1}^{B}
                \Bigl(\mathcal{L}^k_{\text{image}} + \mathcal{L}^k_{\text{text}}\Bigr),
          \]

          \[
            \mathcal{L}^k_{\text{image}}
              = -\log
                \frac{\exp\Bigl(\frac{i_k \cdot t_k}{\tau}\Bigr)}
                      {\sum_{j=1}^{B} \exp\Bigl(\frac{i_k \cdot t_j}{\tau}\Bigr)},
            \quad
            \mathcal{L}^k_{\text{text}}
              = -\log
                \frac{\exp\Bigl(\frac{i_k \cdot t_k}{\tau}\Bigr)}
                      {\sum_{j=1}^{B} \exp\Bigl(\frac{i_j \cdot t_k}{\tau}\Bigr)}.
          \]

          <!-- (2) CMA 수식 -->
          \[
            \mathcal{L}^k_{\text{imageCMA}}
              = -\log \sum_{j=1}^{B} \exp\Bigl(\frac{i_k \cdot t_j}{\tau}\Bigr),
            \quad
            \mathcal{L}^k_{\text{textCMA}}
              = -\log \sum_{j=1}^{B} \exp\Bigl(\frac{i_j \cdot t_k}{\tau}\Bigr).
          \]

          \[
            \mathcal{L}_{CMA}
              = \mathcal{L}_{CLIP}
                + \frac{\lambda}{2B}
                  \sum_{k=1}^{B} 
                  \Bigl(\mathcal{L}^k_{\text{imageCMA}} + \mathcal{L}^k_{\text{textCMA}}\Bigr).
          \]
        </div>
        
        <div class="content has-text-justified">
          <p>
            To implement these ideas, we employ a contrastive loss for each modality with an additional CMA regularization loss. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
